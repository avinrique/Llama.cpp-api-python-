# Llama.cpp-api-python-
A simple python code to get the llama.cpp chat api

## First clone and run llama.cpp library from 
<a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>
<p>After all done , navigate to the llama.cpp folder</p>
### In llama.cpp run the server file this can be done using the command 
####  ./server -m models/13b-chat/ggml-model-q4_0.bin -c 2048
<p>here edit the path to that of the ggml file </p>
###After running the server now clone this repo
<a href="https://github.com/avinrique/Llama.cpp-api-python-/">https://github.com/avinrique/Llama.cpp-api-python-/</a>
command
git clone https://github.com/avinrique/Llama.cpp-api-python-
cd Llama.cpp-api-python-
python 
